# ğŸ”¬ Pipeline d'Identification MS


## Table des matiÃ¨res
1. [Introduction](#1-introduction-)
2. [Installation](#2-installation-%EF%B8%8F)
3. [Structure et Configuration](#3-structure-et-configuration-)
4. [Composants Principaux](#4-composants-principaux-)
5. [Utilisation](#5-utilisation-)
6. [RÃ©sultats et Visualisation](#6-rÃ©sultats-et-visualisation-)
7. [Licence](#7-licence-%E2%9A%96%EF%B8%8F)
8. [Citation](#8-citation)


## 1. Introduction ğŸš€

Pipeline d'identification de composÃ©s intÃ©grant MS1, mobilitÃ© ionique (CCS) et MS2, avec filtration par rÃ©plicats, soustraction des blancs, et alignement des Ã©chantillons.

La pipepline travaille sur trois dimensions analytiques :
- La masse exacte (m/z)
- Le temps de rÃ©tention (RT)  
- Le temps de dÃ©rive (DT)

La pipeline est structurÃ©e en modules interconnectÃ©s, chacun responsable d'une Ã©tape spÃ©cifique du traitement des donnÃ©es.

**Flux de DonnÃ©es**

1. DÃ©tection des pics MS1 dans les donnÃ©es brutes
2. Traitement des rÃ©plicats pour valider les pics dÃ©tectÃ©s
3. Soustraction des blancs pour Ã©liminer les contaminations
4. Calcul des valeurs CCS via la calibration
5. Alignement des features entre Ã©chantillons
6. Identification des composÃ©s et validation MS2

**Composants Principaux**

- **PeakDetector** : dÃ©tection des pics dans les donnÃ©es brutes
- **ReplicateProcessor** : gestion et validation des rÃ©plicats
- **BlankProcessor** : traitement et soustraction des blancs
- **CCSCalibrator** : calibration et calcul des CCS
- **FeatureProcessor** : alignement des features entre Ã©chantillons
- **CompoundIdentifier** : identification des composÃ©s
- **MS2Extractor** : extraction et validation des spectres MS2

Chaque composant peut Ãªtre utilisÃ© indÃ©pendamment ou dans le flux complet de la pipeline.


## 2. Installation âš™ï¸

### 2.1. Via Conda (RecommandÃ©)
```bash
# Cloner le repository
git clone https://github.com/votre_username/pipeline_identification_ms.git
cd pipeline_identification_ms

# CrÃ©er et activer l'environnement
conda env create -f environment.yml
conda activate ms_pipeline

# VÃ©rifier l'installation
python -c "import deimos; print(deimos.__version__)"
```

En cas d'erreur avec DEIMoS :
```bash
conda activate ms_pipeline
pip uninstall deimos
pip install git+https://github.com/pnnl/deimos.git
```

### 2.2. Via Pip
```bash
# CrÃ©er un environnement virtuel
python -m venv ms_env

# Activer l'environnement
# Sur Windows :
ms_env\Scripts\activate
# Sur Linux/macOS :
source ms_env/bin/activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### 2.3. PrÃ©requis

**Fichiers Requis**
- ğŸ“¥ Base de donnÃ©es NORMAN ([TÃ©lÃ©charger ici](https://drive.google.com/file/d/1mZa1r9RZ4Ioy1cILJqIteAz3vUs_UIaU/view))
- ğŸ—‚ï¸ Fichiers blancs dans `data/input/blanks/`

**Structure Ã  CrÃ©er**
```
data/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ samples/          # Vos fichiers .parquet
â”‚   â”œâ”€â”€ blanks/          # Vos fichiers blancs
â”‚   â”œâ”€â”€ calibration/     # Fichiers de calibration
â”‚   â””â”€â”€ databases/       # Base NORMAN
```

**Configuration SystÃ¨me**
- Python 3.8 ou supÃ©rieur
- 8 Go RAM minimum recommandÃ©
  

## 3. Structure et Configuration ğŸ“

### 3.1. Organisation du Projet

```
pipeline_identification_ms/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/  
â”‚   â”‚   â”œâ”€â”€ samples/        # Fichiers d'Ã©chantillons (.parquet)
â”‚   â”‚   â”œâ”€â”€ blanks/         # Fichiers de blancs
â”‚   â”‚   â”œâ”€â”€ calibration/    # Fichiers de calibration CCS
â”‚   â”‚   â””â”€â”€ databases/      # Base de donnÃ©es NORMAN
â”‚   â”‚
â”‚   â”œâ”€â”€ intermediate/       # RÃ©sultats intermÃ©diaires
â”‚   â”‚   â””â”€â”€ samples/
â”‚   â”‚       â””â”€â”€ nom_echantillon/
â”‚   â”‚           â”œâ”€â”€ ms1/
â”‚   â”‚           â”‚   â”œâ”€â”€ peaks_before_blank.parquet
â”‚   â”‚           â”‚   â””â”€â”€ common_peaks.parquet
â”‚   â”‚           â””â”€â”€ ms2/
â”‚   â”‚               â””â”€â”€ spectra.parquet
â”‚   â”‚
â”‚   â””â”€â”€ output/            # RÃ©sultats finaux
â”‚       â”œâ”€â”€ feature_matrix.parquet
â”‚       â”œâ”€â”€ feature_matrix.csv
â”‚       â””â”€â”€ features_complete.parquet
â”‚
â”œâ”€â”€ scripts/               # Code source
â”‚   â”œâ”€â”€ processing/        # Modules de traitement
â”‚   â”‚   â”œâ”€â”€ peak_detection.py
â”‚   â”‚   â”œâ”€â”€ blank_processing.py
â”‚   â”‚   â”œâ”€â”€ replicate_processing.py
â”‚   â”‚   â”œâ”€â”€ ccs_calibration.py
â”‚   â”‚   â”œâ”€â”€ feature_matrix.py
â”‚   â”‚   â”œâ”€â”€ identification.py
â”‚   â”‚   â””â”€â”€ ms2_extraction.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/            # Fonctions utilitaires
â”‚       â”œâ”€â”€ io_handlers.py
â”‚       â”œâ”€â”€ matching_utils.py
â”‚       â””â”€â”€ replicate_handling.py
â”‚
â””â”€â”€ logs/                 # Fichiers de logs
```

**Description des Dossiers**

- `data/input/` : Contient toutes les donnÃ©es d'entrÃ©e nÃ©cessaires
  - `samples/` : Fichiers d'Ã©chantillons au format .parquet
  - `blanks/` : Fichiers de blancs analytiques
  - `calibration/` : DonnÃ©es pour la calibration CCS
  - `databases/` : Base de donnÃ©es de rÃ©fÃ©rence

- `data/intermediate/` : Stocke les rÃ©sultats de chaque Ã©tape
  - Organisation par Ã©chantillon
  - SÃ©paration MS1/MS2
  - RÃ©sultats avant/aprÃ¨s soustraction des blancs

- `data/output/` : Contient les rÃ©sultats finaux
  - Matrices de features
  - Identifications
  - Fichiers aux formats .parquet et .csv

- `scripts/` : Code source de la pipeline
  - `processing/` : Modules principaux de traitement
  - `utils/` : Fonctions utilitaires et helpers


- Fichiers d'Ã©chantillons : `nom_echantillon.parquet`
- RÃ©plicats : `nom_echantillon_replicate_1_X.parquet`
- Blancs : `blank_replicate_1.parquet`
- RÃ©sultats : `nom_explicite.parquet/csv`


### 3.2. Configuration

La configuration de la pipeline est gÃ©rÃ©e par des classes dÃ©diÃ©es dans `config.py`. Chaque aspect du traitement a sa propre configuration avec des paramÃ¨tres par dÃ©faut modifiables.

**ParamÃ¨tres Globaux**
- Organisation en classes de configuration
- Gestion des chemins automatisÃ©e
- Configuration du traitement parallÃ¨le
- ParamÃ¨tres clustering

**TolÃ©rances d'Identification**
- ParamÃ¨tres MS1 et MS2
- TolÃ©rances pour l'alignement
- Seuils de validation

**Optimisation des Performances**
- Nombre de workers parallÃ¨les
- Taille des lots de traitement
- Seuils d'intensitÃ©

```python
class Config:
    """Configuration du pipeline"""
    
    # DÃ©tection des pics
    PEAK_DETECTION = PeakDetectionConfig(
        threshold = 100,
        smooth_iterations = 7,
        smooth_radius = {
            'mz': 0,
            'drift_time': 1,
            'retention_time': 0
        },
        peak_radius = {
            'mz': 2,
            'drift_time': 10,
            'retention_time': 0
        }
    )

    # Identification des composÃ©s
    IDENTIFICATION = IdentificationConfig(
        database_file = "norman_all_ccs_all_rt_pos_neg_with_ms2.h5",
        database_key = "positive",
        tolerances = {
            'mz_ppm': 5,
            'ccs_percent': 8,
            'rt_min': 0.1
        }
    )

    # Soustraction des blancs
    BLANK_SUBTRACTION = BlankSubtractionConfig(
        mz_ppm = 10,
        dt_tolerance = 0.22,
        rt_tolerance = 0.1,
        dbscan_eps = 1.5,
        dbscan_min_samples = 2
    )

    # Traitement des rÃ©plicats
    REPLICATE = ReplicateConfig(
        min_replicates = 2,
        mz_ppm = 10,
        dt_tolerance = 0.22,
        rt_tolerance = 0.1
    )

    # Extraction MS2
    MS2_EXTRACTION = MS2ExtractionConfig(
        rt_tolerance = 0.00422,
        dt_tolerance = 0.22,
        max_peaks = 10,
        intensity_scale = 999
    )
```

**Modules Configurables**
- PeakDetection : dÃ©tection des pics MS1
- Identification : paramÃ¨tres d'identification
- BlankSubtraction : soustraction des blancs
- Replicate : gestion des rÃ©plicats
- MS2Extraction : extraction des spectres MS2
- Alignement : Alignement des Ã©chantillons


## 4. Composants Principaux ğŸ”§

### 4.1. DÃ©tection des Pics (PeakDetector)

Le PeakDetector est responsable de la dÃ©tection des pics MS1 dans les donnÃ©es brutes. Il intÃ¨gre le lissage des donnÃ©es, la dÃ©tection des pics et le clustering intra-Ã©chantillon.

**Utilisation Basique**
```python
from scripts.processing.peak_detection import PeakDetector

# Initialisation
detector = PeakDetector()

# Traitement simple
peaks = detector.process_sample(data)

# Traitement avec paramÃ¨tres personnalisÃ©s
peaks = detector.detect_peaks(
    data,
    threshold=200,              # Seuil plus Ã©levÃ©
    smooth_iterations=7         # Plus d'itÃ©rations de lissage
)
```

**ParamÃ¨tres**
```python
# Configuration par dÃ©faut
PEAK_DETECTION = {
    'threshold': 50,           # Seuil d'intensitÃ© minimum
    'smooth_iterations': 7,     # Nombre d'itÃ©rations pour le lissage
    'smooth_radius': {          # Rayons de lissage
        'mz': 0,
        'drift_time': 1,
        'retention_time': 0
    },
    'peak_radius': {           # Rayons pour la dÃ©tection
        'mz': 2,
        'drift_time': 10,
        'retention_time': 0
    }
}
```

**Workflow de Traitement**
1. PrÃ©paration des donnÃ©es
   ```python
   prepared_data = detector.prepare_data(data)
   ```

2. DÃ©tection des pics
   ```python
   peaks = detector.detect_peaks(prepared_data)
   ```

3. Clustering des pics
   ```python
   clustered_peaks = detector.cluster_peaks(peaks)
   ```

**Format des DonnÃ©es de Sortie**
```python
# Exemple de DataFrame retournÃ©
peaks_df = {
    'mz': [123.4567, ...],           # Masse exacte
    'drift_time': [12.3, ...],       # Temps de dÃ©rive
    'retention_time': [5.6, ...],    # Temps de rÃ©tention
    'intensity': [1000, ...]         # IntensitÃ© du pic
}
```

### 4.2. Traitement des RÃ©plicats (ReplicateProcessor)

Le ReplicateProcessor valide la prÃ©sence des pics entre diffÃ©rents rÃ©plicats d'un mÃªme Ã©chantillon pour assurer la fiabilitÃ© des rÃ©sultats.

**Utilisation Basique**
```python
from scripts.processing.replicate_processing import ReplicateProcessor
from pathlib import Path

# Initialisation
processor = ReplicateProcessor()

# Liste des fichiers rÃ©plicats
replicate_files = [
    Path("data/input/samples/sample_replicate_1.parquet"),
    Path("data/input/samples/sample_replicate_1_2.parquet"),
    Path("data/input/samples/sample_replicate_1_3.parquet")
]

# Traitement des rÃ©plicats
results = processor.process_sample_with_replicates(
    sample_name="mon_echantillon",
    replicate_files=replicate_files,
    output_dir=Path("data/output")
)
```

**Configuration**
```python
REPLICATE = {
    'min_replicates': 2,       # Nombre minimum de rÃ©plicats requis
    'mz_ppm': 10,             # TolÃ©rance m/z en ppm
    'dt_tolerance': 0.22,      # TolÃ©rance temps de dÃ©rive
    'rt_tolerance': 0.1,       # TolÃ©rance temps de rÃ©tention
    'dbscan_eps': 0.6,        # Epsilon pour DBSCAN
    'dbscan_min_samples': 2    # Ã‰chantillons minimum pour DBSCAN
}
```

**Workflow de Traitement**
1. Traitement individuel des rÃ©plicats
```python
peaks_dict, initial_counts = processor.process_replicates(replicate_files)
```

2. Clustering entre rÃ©plicats
```python
common_peaks = processor.cluster_replicates(peaks_dict)
```

**Format de Sortie**
```python
# Structure des rÃ©sultats
results = {
    'mz': [],                 # Masses moyennes
    'drift_time': [],         # Temps de dÃ©rive moyens
    'retention_time': [],     # Temps de rÃ©tention moyens
    'intensity': [],          # IntensitÃ©s maximales
    'n_replicates': []        # Nombre de rÃ©plicats oÃ¹ le pic est trouvÃ©
}
```

### 4.3. Soustraction des Blancs (BlankProcessor)

Le BlankProcessor Ã©limine les contaminations et le bruit de fond en soustrayant les pics dÃ©tectÃ©s dans les blancs analytiques.

**Utilisation Basique**
```python
from scripts.processing.blank_processing import BlankProcessor
from pathlib import Path

# Initialisation
blank_processor = BlankProcessor()

# Traitement d'un fichier blank individuel
blank_peaks = blank_processor.process_blank_file(blank_file)

# Soustraction des blancs des Ã©chantillons
clean_peaks = blank_processor.subtract_blank_peaks(sample_peaks, blank_peaks)
```

**MÃ©thodologie**
1. Traitement des Blancs
   - DÃ©tection des pics dans les blancs
   - Regroupement des rÃ©plicats de blancs
   - Validation des pics communs

2. Soustraction
   - Comparaison des pics Ã©chantillon/blanc
   - Application des tolÃ©rances
   - Filtrage des pics contaminants

**Configuration**
```python
BLANK_SUBTRACTION = {
    'mz_ppm': 10,              # TolÃ©rance masse
    'dt_tolerance': 0.22,      # TolÃ©rance temps de dÃ©rive
    'rt_tolerance': 0.1,       # TolÃ©rance temps de rÃ©tention
    'dbscan_eps': 1.5,         # Epsilon clustering
    'dbscan_min_samples': 2,   # Minimum Ã©chantillons
}
```

**Format des DonnÃ©es de Sortie**
```python
clean_peaks = {
    'mz': [],                  # Masses validÃ©es
    'drift_time': [],          # Temps de dÃ©rive
    'retention_time': [],      # Temps de rÃ©tention
    'intensity': []            # IntensitÃ©s
}
```

### 4.4. Features et Alignement (FeatureProcessor)

Le FeatureProcessor aligne les pics entre diffÃ©rents Ã©chantillons pour crÃ©er une matrice de features commune. Il permet d'identifier et de quantifier les composÃ©s Ã  travers tous les Ã©chantillons.

**Utilisation Basique**
```python
from scripts.processing.feature_matrix import FeatureProcessor
from pathlib import Path

# Initialisation
processor = FeatureProcessor()

# Alignement et crÃ©ation de matrice
matrix, feature_info, raw_files = processor.align_features_across_samples(
    samples_dir=Path("data/intermediate/samples")
)

# Traitement des features avec identification
identifications = processor.process_features(
    feature_df=feature_info,
    raw_files=raw_files,
    identifier=identifier  # Instance de CompoundIdentifier
)
```

**Configuration**
```python
FEATURE_ALIGNMENT = {
    'mz_ppm': 10,              # TolÃ©rance masse
    'dt_tolerance': 1.02,      # TolÃ©rance temps de dÃ©rive
    'rt_tolerance': 0.2,       # TolÃ©rance temps de rÃ©tention
    'dbscan_eps': 1.0,         # Epsilon pour clustering
    'dbscan_min_samples': 1    # Minimum d'Ã©chantillons
}
```

**CrÃ©ation de Matrices**
1. Matrice d'IntensitÃ©s
```python
# Format de la matrice
matrix_df = pd.DataFrame(
    index=sample_names,        # Ã‰chantillons en lignes
    columns=feature_names      # Features en colonnes
)
```

2. Matrice de Features
```python
# Informations sur les features
feature_df = {
    'mz': [],                  # Masse moyenne
    'retention_time': [],      # RT moyen
    'drift_time': [],          # DT moyen
    'intensity': [],           # IntensitÃ© maximale
    'n_samples': [],           # Nombre d'Ã©chantillons
    'feature_id': []           # Identifiant unique
}
```

**Sorties GÃ©nÃ©rÃ©es**
```
data/output/
â”œâ”€â”€ feature_matrix.parquet     # Matrice d'intensitÃ©s
â”œâ”€â”€ feature_matrix.csv        
â”œâ”€â”€ features_complete.parquet  # Informations dÃ©taillÃ©es
â””â”€â”€ features_complete.csv
```


### 4.5. Calibration CCS (CCSCalibrator)

Le CCSCalibrator permet de calculer les valeurs CCS (Section Efficace de Collision) Ã  partir des temps de dÃ©rive mesurÃ©s.

**Utilisation Basique**
```python
from scripts.processing.ccs_calibration import CCSCalibrator

# Initialisation avec fichier de calibration
calibrator = CCSCalibrator("path/to/calibration.csv")

# Calibration
calibrator.calibrate()

# Calcul des CCS pour les pics
ccs_values = calibrator.calculate_ccs(peaks_df)
```

**Processus de Calibration**
1. Chargement des donnÃ©es
```python
# Format du fichier de calibration
calibration_data = {
    'Reference m/z': [],    # m/z de rÃ©fÃ©rence
    'Measured m/z': [],     # m/z mesurÃ©
    'Measured Time': [],    # Temps de dÃ©rive mesurÃ©
    'Reference rCCS': [],   # CCS de rÃ©fÃ©rence
    'z': []                # Ã‰tat de charge
}
```

### 4.6. Identification (CompoundIdentifier)

Le CompoundIdentifier compare les pics dÃ©tectÃ©s avec une base de donnÃ©es de rÃ©fÃ©rence pour identifier les composÃ©s.

**Utilisation Basique**
```python
from scripts.processing.identification import CompoundIdentifier

# Initialisation
identifier = CompoundIdentifier()

# Identification des composÃ©s
matches = identifier.identify_compounds(peaks_df, output_dir)
```

**Configuration Base de DonnÃ©es**
```python
IDENTIFICATION = {
    'database_file': "norman_all_ccs_all_rt_pos_neg_with_ms2.h5",
    'database_key': "positive",
    'tolerances': {
        'mz_ppm': 5,           # TolÃ©rance masse
        'ccs_percent': 8,      # TolÃ©rance CCS
        'rt_min': 0.1          # TolÃ©rance RT
    },
    'ms2_score_threshold': 0.5 # Seuil score MS2
}
```

**Format des RÃ©sultats**
```python
matches = {
    'match_name': [],         # Nom du composÃ©
    'formula': [],           # Formule molÃ©culaire
    'confidence_level': [],   # Niveau de confiance
    'global_score': [],      # Score global
    'ms2_score': []          # Score MS2 si disponible
}
```

### 4.7. MS2 (MS2Extractor)

Le MS2Extractor extrait et traite les spectres MS2 pour valider les identifications via la comparaison avec des spectres de rÃ©fÃ©rence.

**Utilisation Basique**
```python
from scripts.processing.ms2_extraction import MS2Extractor

# Initialisation
extractor = MS2Extractor()

# Extraction d'un spectre MS2
spectra = extractor.extract_ms2_spectrum(
    ms2_data=ms2_data,
    rt=retention_time,
    dt=drift_time
)

# Extraction pour plusieurs matches
matches_with_ms2 = extractor.extract_ms2_for_matches(
    matches_df=matches,
    raw_parquet_path="path/to/raw.parquet",
    output_dir="path/to/output"
)
```

**Configuration**
```python
MS2_EXTRACTION = {
    'rt_tolerance': 0.00422,    # FenÃªtre RT (minutes)
    'dt_tolerance': 0.22,       # FenÃªtre DT (ms)
    'mz_round_decimals': 3,     # PrÃ©cision m/z
    'max_peaks': 10,            # Pics maximum
    'intensity_scale': 999      # Ã‰chelle d'intensitÃ©
}
```

## 5. Utilisation ğŸ“Š

Pour utiliser la pipeline, il suffit de suivre ces Ã©tapes :

**1. PrÃ©paration des DonnÃ©es**

Placez vos fichiers dans les dossiers appropriÃ©s :
```
data/
â”œâ”€â”€ input/
â”‚   â”œâ”€â”€ samples/          # Vos Ã©chantillons (.parquet)
â”‚   â”œâ”€â”€ blanks/          # Vos blancs
â”‚   â”œâ”€â”€ calibration/     # Fichier de calibration CCS
â”‚   â””â”€â”€ databases/       # Base NORMAN
```

**2. Lancement de la Pipeline**

Depuis le terminal, dans le dossier du projet :
```bash
# Activation de l'environnement
conda activate ms_pipeline

# Lancement de la pipeline
python main.py
```

**3. Options Disponibles**
```bash
# Aide sur les options
python main.py --help

# SpÃ©cifier un dossier d'entrÃ©e diffÃ©rent
python main.py --input_dir "chemin/vers/donnÃ©es"

# SpÃ©cifier un dossier de sortie
python main.py --output_dir "chemin/vers/sortie"
```

**4. Suivi du Traitement**

La pipeline affiche sa progression :
```
ğŸš€ DÃ©marrage du traitement...
   
ğŸ“Š Traitement des blancs...
âœ“ Blanc 1 traitÃ©
âœ“ Blanc 2 traitÃ©

ğŸ” Traitement des Ã©chantillons...
âœ“ Ã‰chantillon 1 (3 rÃ©plicats)
âœ“ Ã‰chantillon 2 (3 rÃ©plicats)
...

âœ¨ Traitement terminÃ© !
```

**5. RÃ©sultats**

Les rÃ©sultats sont automatiquement sauvegardÃ©s dans `data/output/` :
- `feature_matrix.parquet` : Matrice d'intensitÃ©s
- `feature_matrix.csv` : Version CSV de la matrice
- `features_complete.parquet` : DonnÃ©es complÃ¨tes avec identifications
- `features_complete.csv` : Version CSV des identifications


## 6. RÃ©sultats et Visualisation ğŸ“ˆ

### 6.1. Formats de Sortie

La pipeline gÃ©nÃ¨re plusieurs fichiers de sortie organisÃ©s de maniÃ¨re structurÃ©e.

**Structure des Dossiers de Sortie**
```
data/output/
â”œâ”€â”€ feature_matrix.parquet     # Matrice principale des features
â”œâ”€â”€ feature_matrix.csv        
â”œâ”€â”€ features_complete.parquet  # DonnÃ©es avec identifications
â””â”€â”€ features_complete.csv     
```

**1. Matrice des Features**
- Format : CSV et Parquet
- Structure :
```python
# feature_matrix.csv
              F001_mz123.45  F002_mz456.78  F003_mz789.01
Sample_1         1234.56       0.00           789.01
Sample_2         1567.89       456.78         0.00
Sample_3         1432.12       567.89         654.32
```

**2. Liste des Identifications**
- Format : CSV et Parquet
- Contenu :
```python
# features_complete.csv
Colonnes principales :
- feature_id       # Identifiant unique
- mz              # Masse mesurÃ©e
- rt              # Temps de rÃ©tention
- drift_time      # Temps de dÃ©rive
- ccs             # Valeur CCS calculÃ©e
- match_name      # Nom du composÃ© identifiÃ©
- formula         # Formule molÃ©culaire
- score           # Score global
- confidence      # Niveau de confiance
```

**Exemple de RÃ©sultats**
| ComposÃ© | m/z mesurÃ© | m/z thÃ©orique | Erreur (ppm) | CCS | RT (min) | Score MS2 | Formule | Adduit |
|---------|------------|---------------|--------------|-----|-----------|-----------|----------|---------|
| CafÃ©ine | 195.0879 | 195.0882 | -1.11 | 143.97 | 4.01 | 0.89 | C8H10N4O2 | [M+H]+ |
| ParacÃ©tamol | 152.0706 | 152.0712 | -3.94 | 131.45 | 3.22 | 0.92 | C8H9NO2 | [M+H]+ |
| IbuprofÃ¨ne | 207.1378 | 207.1380 | -0.96 | 152.88 | 8.45 | 0.78 | C13H18O2 | [M+H]+ |

**Formats Disponibles**
- `.parquet` : Format optimisÃ© pour l'analyse ultÃ©rieure
- `.csv` : Format lisible et compatible avec Excel


**AccÃ¨s aux RÃ©sultats**
```python
# Lecture des rÃ©sultats
import pandas as pd

# Matrice des features
matrix = pd.read_parquet("data/output/feature_matrix.parquet")

# Identifications complÃ¨tes
identifications = pd.read_parquet("data/output/features_complete.parquet")
```

**Analyse des RÃ©sultats**

La matrice de features (`feature_matrix.csv/parquet`) contient :
- Les intensitÃ©s de chaque feature dans tous les Ã©chantillons
- Format : Ã©chantillons en lignes, features en colonnes
- Nomenclature : `FXXX_mzYYY.YYYY` oÃ¹ :
  - XXX : numÃ©ro unique de la feature
  - YYY.YYYY : masse exacte mesurÃ©e

Dans `features_complete.csv/parquet` :
- Toutes les features identifiÃ©es
- ParamÃ¨tres analytiques (m/z, RT, DT, CCS)
- Informations d'identification (nom, formule, score)


## 7. Licence âš–ï¸

Ce projet est sous licence [CC BY-NC 4.0](https://creativecommons.org/licenses/by-nc/4.0/).

### 8. Citation

Pour citer ce projet dans une publication acadÃ©mique, veuillez utiliser :

```bibtex
@software{pipeline_identification_ms,
    title = {Pipeline d'Identification MS},
    year = {2024},
    author = {Sade, Julien},
    url = {https://github.com/narvall018/pipeline_identification_ms},
    version = {1.0.0},
    institution = {Leesu},
    note = {Pipeline pour l'identification de composÃ©s par spectromÃ©trie de masse}
}
```

Pour une citation dans le texte :
> Sade, J. (2024). Pipeline d'Identification MS,https://github.com/narvall018/pipeline_identification_ms

**Contact**

Pour toute question concernant l'utilisation :
- âœ‰ï¸ julien.sade@u-pec.fr

