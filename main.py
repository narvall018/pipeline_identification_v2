#main.py
#-*- coding:utf-8 -*--

import gc 
import logging
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import warnings
import pandas as pd
from pathlib import Path
from io import StringIO
import sys
import time
from tqdm import tqdm
import matplotlib.pyplot as plt 
from typing import Dict, Union, List
import psutil


# Configuration
from scripts.config.config import Config

# Classes pour le traitement
from scripts.processing.peak_detection import PeakDetector
from scripts.processing.blank_processing import BlankProcessor
from scripts.processing.replicate_processing import ReplicateProcessor
from scripts.processing.ccs_calibration import CCSCalibrator
from scripts.processing.identification import CompoundIdentifier
from scripts.processing.feature_matrix import FeatureProcessor
from scripts.utils.replicate_handling import ReplicateHandler


# Utilitaires
from scripts.utils.io_handlers import IOHandler
from scripts.utils.replicate_handling import ReplicateHandler

# Visualisations 
from scripts.visualization.plotting import (
    plot_unique_molecules_per_sample,
    plot_level1_molecules_per_sample,
    plot_sample_similarity_heatmap,
    plot_sample_similarity_heatmap_by_confidence,
    plot_level1_molecule_distribution_bubble,
    analyze_sample_clusters,
    plot_cluster_statistics,
    analyze_and_save_clusters,
    plot_tics_interactive,
    analyze_categories
)

# Suppression des warnings pandas
warnings.filterwarnings('ignore')
pd.options.mode.chained_assignment = None

# Initialiser le logger
logger = logging.getLogger(__name__)

def setup_logging() -> None:
    """Configure le syst√®me de logging."""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)
    logging.basicConfig(
        filename=log_dir / "peak_detection.log",
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        force=True
    )

class CaptureOutput:
    """Capture la sortie standard pour chaque processus."""
    def __init__(self):
        self.output = StringIO()
        self.stdout = sys.stdout
        
    def __enter__(self):
        sys.stdout = self.output
        return self.output
        
    def __exit__(self, *args):
        sys.stdout = self.stdout

class SampleResult:
    """Stocke les r√©sultats du traitement d'un √©chantillon."""
    def __init__(self, name: str, peaks_df: pd.DataFrame, processing_time: float, logs: str):
        self.name = name
        self.peaks_df = peaks_df
        self.processing_time = processing_time
        self.logs = logs
        self.success = not peaks_df.empty
        
        # Extraire les statistiques des logs
        self.initial_peaks = 0
        self.after_clustering = 0
        self.after_blank = len(peaks_df) if not peaks_df.empty else 0
        self.final_peaks = len(peaks_df) if not peaks_df.empty else 0
        
        for line in logs.split('\n'):
            if "Pics initiaux:" in line:
                try:
                    self.initial_peaks = int(line.split(": ")[1])
                except:
                    pass
            elif "apr√®s clustering:" in line and "apr√®s clustering: {len" not in line:
                try:
                    self.after_clustering = int(line.split(": ")[1])
                except:
                    pass
            elif "Pics apr√®s soustraction" in line:
                try:
                    self.after_blank = int(line.split(": ")[1])
                except:
                    pass

def process_single_sample(
    args: tuple
) -> tuple:
    """Traite un seul √©chantillon en capturant sa sortie."""
    base_name, replicates, blank_peaks, calibrator, output_base_dir = args
    start_time = time.time()
    
    with CaptureOutput() as output:
        try:
            # Instanciation des processeurs
            replicate_processor = ReplicateProcessor()
            blank_processor = BlankProcessor()
            
            # 1. Traitement des r√©plicats
            common_peaks = replicate_processor.process_sample_with_replicates(
                base_name,
                replicates,
                output_base_dir
            )
            
            if common_peaks.empty:
                print(f"‚úó Pas de pics trouv√©s pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 2. Soustraction du blank 
            if not blank_peaks.empty:
                clean_peaks = blank_processor.subtract_blank_peaks(common_peaks, blank_peaks)
            else:
                clean_peaks = common_peaks
                
            if clean_peaks.empty:
                print(f"‚úó Pas de pics apr√®s soustraction du blank pour {base_name}")
                return base_name, SampleResult(
                    base_name, pd.DataFrame(), 
                    time.time() - start_time, 
                    output.getvalue()
                )
            
            # 3. Calibration CCS 
            peaks_with_ccs = calibrator.calculate_ccs(clean_peaks)
            
            if not peaks_with_ccs.empty:
                print(f"‚úì CCS calcul√©es pour {len(peaks_with_ccs)} pics")
                print(f"‚úì Plage de CCS: {peaks_with_ccs['CCS'].min():.2f} - {peaks_with_ccs['CCS'].max():.2f} √Ö¬≤")
                print(f"‚úì CCS moyenne: {peaks_with_ccs['CCS'].mean():.2f} √Ö¬≤")
                
                # Sauvegarde
                io_handler = IOHandler()
                output_dir = output_base_dir / base_name / "ms1"
                output_dir.mkdir(parents=True, exist_ok=True)
                output_file = output_dir / "common_peaks.parquet"
                io_handler.save_results(peaks_with_ccs, output_file)
                
            return base_name, SampleResult(
                base_name, peaks_with_ccs,
                time.time() - start_time,
                output.getvalue()
            )
            
        except Exception as e:
            print(f"‚ùå Erreur lors du traitement de {base_name}: {str(e)}")
            return base_name, SampleResult(
                base_name, pd.DataFrame(),
                time.time() - start_time,
                output.getvalue()
            )

def generate_molecules_per_sample(output_dir: Path):
    fig = plot_unique_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_molecules(output_dir: Path):
    fig = plot_level1_molecules_per_sample(output_dir)
    fig.savefig(output_dir / "level1_molecules_per_sample.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_bubble_plot(output_dir: Path):
    fig = plot_level1_molecule_distribution_bubble(output_dir)
    fig.savefig(output_dir / "level1_molecule_distribution_bubble.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_similarity_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap(output_dir)
    fig.savefig(output_dir / "sample_similarity_heatmap_all.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level1_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1],
        title_suffix=" - Niveau 1"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level1.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level12_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2],
        title_suffix=" - Niveaux 1 et 2"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level12.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_level123_heatmap(output_dir: Path):
    fig = plot_sample_similarity_heatmap_by_confidence(
        output_dir, 
        confidence_levels=[1, 2, 3],
        title_suffix=" - Niveaux 1, 2 et 3"
    )
    fig.savefig(output_dir / "sample_similarity_heatmap_level123.png", bbox_inches='tight', dpi=300)
    plt.close(fig)

def generate_tics(output_dir: Path):
    plot_tics_interactive(Path("data/input/samples"), output_dir)

def generate_visualizations(output_dir: Path) -> None:
    """G√©n√®re toutes les visualisations de la pipeline en parall√®le."""
    try:
        print("\nüìä G√©n√©ration des visualisations...")
        output_dir.mkdir(exist_ok=True)
        
        # V√©rifier l'existence du fichier d'identifications dans le sous-dossier feature_matrix
        identifications_file = output_dir / "feature_matrix" / "features_complete.parquet"
        if not identifications_file.exists():
            raise FileNotFoundError(f"Fichier d'identifications non trouv√©: {identifications_file}")

        # Liste des t√¢ches √† ex√©cuter avec leurs arguments
        tasks = [
            (generate_molecules_per_sample, output_dir),
            (generate_level1_molecules, output_dir),
            (generate_bubble_plot, output_dir),
            (generate_similarity_heatmap, output_dir),
            (generate_level1_heatmap, output_dir),
            (generate_level12_heatmap, output_dir),
            (generate_level123_heatmap, output_dir)
            #,(generate_tics, output_dir)
        ]

        # Ex√©cution parall√®le des t√¢ches
        max_workers = min(mp.cpu_count(), len(tasks))

        futures_to_task = {}
        with ProcessPoolExecutor(max_workers=2) as executor:
            # Soumettre toutes les t√¢ches
            for func, arg in tasks:
                future = executor.submit(func, arg)
                futures_to_task[future] = func.__name__
            
            # Attendre leur compl√©tion avec une barre de progression
            with tqdm(total=len(futures_to_task), desc="G√©n√©ration des visualisations") as pbar:
                for future in as_completed(futures_to_task):
                    task_name = futures_to_task[future]
                    try:
                        future.result()
                    except Exception as e:
                        print(f"Erreur lors de la g√©n√©ration de {task_name}: {str(e)}")
                    pbar.update(1)

        print(f"   ‚úì Visualisations sauvegard√©es dans {output_dir}")

    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation des visualisations: {str(e)}")
        raise

def process_samples_parallel(
    replicate_groups: Dict[str, List[Path]],
    blank_peaks: pd.DataFrame,
    calibrator: CCSCalibrator,
    output_base_dir: Path,
    max_workers: int = None
) -> Dict[str, SampleResult]:
    """
    Traite tous les √©chantillons en parall√®le avec une gestion optimis√©e de la m√©moire.
    
    Args:
        replicate_groups: Dictionnaire des groupes de r√©plicats
        blank_peaks: DataFrame des pics du blank
        calibrator: Instance du calibrateur CCS
        output_base_dir: R√©pertoire de sortie
        max_workers: Nombre maximum de workers
        
    Returns:
        Dict[str, SampleResult]: R√©sultats par √©chantillon
    """
    if max_workers is None:
        max_workers = min(2, mp.cpu_count())

    total_samples = len(replicate_groups)    
    print("\n" + "="*80)
    print(f"TRAITEMENT DES √âCHANTILLONS ({total_samples} √©chantillons)")
    print("="*80)
    
    # Analyse pr√©liminaire des tailles d'√©chantillons
    print("\nAnalyse des tailles d'√©chantillons...")
    sample_sizes = {}
    for base_name, replicates in replicate_groups.items():
        total_size = 0
        for rep_file in replicates:
            file_size = rep_file.stat().st_size
            # Estimation de la m√©moire n√©cessaire (facteur 3 pour le traitement)
            memory_needed = file_size * 3
            total_size += memory_needed
        sample_sizes[base_name] = total_size
        print(f"   ‚Ä¢ {base_name}: {total_size / 1024**2:.1f} MB estim√©s")
    
    # Calcul de la taille de lot bas√©e sur la m√©moire disponible
    available_memory = psutil.virtual_memory().available
    target_memory_usage = available_memory * 0.7  # Utilise 70% de la RAM disponible
    
    # Tri des √©chantillons par taille
    sorted_samples = sorted(sample_sizes.items(), key=lambda x: x[1], reverse=True)
    
    # Cr√©ation de lots √©quilibr√©s
    batches = []
    current_batch = []
    current_batch_size = 0
    
    for sample_name, sample_size in sorted_samples:
        if current_batch_size + sample_size > target_memory_usage or len(current_batch) >= max_workers:
            if current_batch:
                batches.append(current_batch)
            current_batch = [(sample_name, replicate_groups[sample_name])]
            current_batch_size = sample_size
        else:
            current_batch.append((sample_name, replicate_groups[sample_name]))
            current_batch_size += sample_size
    
    if current_batch:
        batches.append(current_batch)
    
    print(f"\nOptimisation des ressources:")
    print(f"   ‚Ä¢ Nombre de lots: {len(batches)}")
    print(f"   ‚Ä¢ Workers: {max_workers}")
    print(f"   ‚Ä¢ M√©moire disponible: {available_memory / 1024**3:.1f} GB")
    print(f"   ‚Ä¢ Utilisation m√©moire cible: {target_memory_usage / 1024**3:.1f} GB")
    
    results = {}
    total_start_time = time.time()
    process = psutil.Process()
    initial_memory = process.memory_info().rss
    
    with tqdm(total=total_samples, unit="√©chantillon") as pbar:
        for batch_idx, batch_items in enumerate(batches, 1):
            print(f"\nTraitement du lot {batch_idx}/{len(batches)} "
                  f"({len(batch_items)} √©chantillons)")
            
            # Force le garbage collector avant chaque batch
            gc.collect()
            
            # Surveillance de la m√©moire
            current_memory = process.memory_info().rss
            memory_increase = current_memory - initial_memory
            if memory_increase > 1e9:  # Si augmentation > 1GB
                print("\n‚ö†Ô∏è Forte utilisation m√©moire d√©tect√©e, nettoyage forc√©...")
                gc.collect()
                initial_memory = process.memory_info().rss
            
            process_args = [
                (base_name, replicates, blank_peaks, calibrator, output_base_dir)
                for base_name, replicates in batch_items
            ]
            
            try:
                with ProcessPoolExecutor(max_workers=max_workers) as executor:
                    future_to_name = {
                        executor.submit(process_single_sample, args): args[0]
                        for args in process_args
                    }
                    
                    for future in as_completed(future_to_name):
                        base_name = future_to_name[future]
                        try:
                            _, result = future.result()
                            
                            # Sauvegarde imm√©diate des r√©sultats sur disque si possible
                            if result.success and hasattr(result, 'peaks_df') and not result.peaks_df.empty:
                                output_dir = output_base_dir / base_name / "ms1"
                                output_dir.mkdir(parents=True, exist_ok=True)
                                result.peaks_df.to_parquet(
                                    output_dir / "processed_peaks.parquet",
                                    compression='snappy'
                                )
                                # Lib√©rer la m√©moire du DataFrame
                                result.peaks_df = None
                                gc.collect()
                            
                            results[base_name] = result
                            
                        except Exception as e:
                            print(f"\n‚ùå Erreur pour {base_name}: {str(e)}")
                            results[base_name] = SampleResult(
                                base_name, 
                                pd.DataFrame(), 
                                0.0,
                                f"Erreur: {str(e)}"
                            )
                        finally:
                            pbar.update(1)
                            
                            # Sauvegarde interm√©diaire des statistiques
                            stats_df = pd.DataFrame([{
                                '√âchantillon': r.name,
                                'Temps (s)': r.processing_time,
                                'Pics initiaux': r.initial_peaks,
                                'Pics apr√®s clustering': r.after_clustering,
                                'Pics apr√®s blank': r.after_blank,
                                'Pics finaux': r.final_peaks,
                                'Statut': 'Succ√®s' if r.success else '√âchec'
                            } for r in results.values()])
                            
                            stats_file = output_base_dir / "processing_statistics.csv"
                            stats_df.to_csv(stats_file, index=False)
                            del stats_df
                            gc.collect()
                
            except Exception as e:
                print(f"\n‚ùå Erreur dans le lot {batch_idx}: {str(e)}")
                for name, _ in batch_items:
                    if name not in results:
                        results[name] = SampleResult(
                            name,
                            pd.DataFrame(),
                            0.0,
                            f"Erreur batch: {str(e)}"
                        )
                        pbar.update(1)
            
            # Pause entre les lots pour lib√©ration m√©moire
            if batch_idx < len(batches):
                #print("\nüí§ Pause pour lib√©ration m√©moire...")
                time.sleep(2)
                gc.collect()
    
    total_time = time.time() - total_start_time
    success_count = sum(1 for r in results.values() if r.success)
    failed_count = len(results) - success_count
    
    # Affichage du r√©capitulatif
    print("\n" + "="*80)
    print("R√âCAPITULATIF")
    print("="*80)
    print(f"\nTemps total: {total_time:.2f} secondes")
    print(f"√âchantillons trait√©s: {success_count}/{len(results)}")
    
    if failed_count > 0:
        print(f"‚ùå √âchantillons en √©chec: {failed_count}")
    
    print("\nD√©tails par √©chantillon:")
    for name, result in results.items():
        status = "‚úì" if result.success else "‚úó"
        print(f"\n{status} {name}")
        print(f"  ‚Ä¢ Temps de traitement: {result.processing_time:.2f}s")
        print(f"  ‚Ä¢ Pics initiaux: {result.initial_peaks}")
        if result.success:
            print(f"  ‚Ä¢ Pics apr√®s clustering: {result.after_clustering}")
            print(f"  ‚Ä¢ Pics apr√®s blank: {result.after_blank}")
            print(f"  ‚Ä¢ Pics finaux: {result.final_peaks}")
        else:
            print(f"  ‚Ä¢ Erreur: {result.logs}")
    
    # Derni√®re sauvegarde des statistiques
    final_stats_df = pd.DataFrame([{
        '√âchantillon': r.name,
        'Temps (s)': r.processing_time,
        'Pics initiaux': r.initial_peaks,
        'Pics apr√®s clustering': r.after_clustering,
        'Pics apr√®s blank': r.after_blank,
        'Pics finaux': r.final_peaks,
        'Statut': 'Succ√®s' if r.success else '√âchec'
    } for r in results.values()])
    
    stats_file = output_base_dir / "processing_statistics.csv"
    final_stats_df.to_csv(stats_file, index=False)
    
    print(f"\nStatistiques sauvegard√©es dans {stats_file}")
    
    return results

def save_intermediate_stats(results: Dict[str, SampleResult], output_dir: Path) -> None:
    """Sauvegarde les statistiques interm√©diaires."""
    stats_df = pd.DataFrame([{
        '√âchantillon': r.name,
        'Temps (s)': r.processing_time,
        'Pics initiaux': r.initial_peaks,
        'Pics apr√®s clustering': r.after_clustering,
        'Pics apr√®s blank': r.after_blank,
        'Pics finaux': r.final_peaks,
        'Statut': 'Succ√®s' if r.success else '√âchec'
    } for r in results.values()])
    
    stats_file = output_dir / "processing_statistics.csv"
    stats_df.to_csv(stats_file, index=False)
    
def print_final_stats(results: Dict[str, SampleResult]) -> None:
    """Affiche les statistiques finales du traitement."""
    success_count = sum(1 for r in results.values() if r.success)
    failed_count = len(results) - success_count
    
    print("\n" + "="*80)
    print("STATISTIQUES FINALES")
    print("="*80)
    print(f"\n‚úì √âchantillons trait√©s avec succ√®s: {success_count}")
    if failed_count > 0:
        print(f"‚úó √âchantillons en √©chec: {failed_count}")
    
    # Statistiques sur la m√©moire
    process = psutil.Process()
    memory_info = process.memory_info()
    print(f"\nUtilisation m√©moire finale: {memory_info.rss / 1024**3:.2f} GB")

def main() -> None:
    """Point d'entr√©e principal de la pipeline."""
    setup_logging()
    start_time = time.time()
    print("\nüöÄ D√âMARRAGE DE LA PIPELINE D'ANALYSE")
    print("=" * 80)

    try:
        # Initialisation des handlers et processeurs
        io_handler = IOHandler()
        replicate_handler = ReplicateHandler()
        blank_processor = BlankProcessor()
        feature_processor = FeatureProcessor()

        # 1. Chargement des donn√©es de calibration CCS
        print("\nüìà Chargement des donn√©es de calibration CCS...")
        calibration_file = Config.PATHS.INPUT_CALIBRANTS / "CCS_calibration_data.csv"
        if not calibration_file.exists():
            raise FileNotFoundError(f"Fichier de calibration non trouv√© : {calibration_file}")
        calibrator = CCSCalibrator(calibration_file)
        print("   ‚úì Donn√©es de calibration charg√©es avec succ√®s")

        # 2. Initialisation de l'identificateur
        print("\nüìö Initialisation de l'identification...")
        identifier = CompoundIdentifier()
        print("   ‚úì Base de donn√©es charg√©e avec succ√®s")
        
        # Recherche des fichiers
        print("\nüìÅ Recherche des blanks...")
        blank_dir = Path("data/input/blanks")
        blank_files = list(blank_dir.glob("*.parquet"))

        # Affichage des informations sur les blanks
        if blank_files:
            print(f"   ‚úì {len(blank_files)} fichier(s) blank trouv√©(s):")
            for blank_file in blank_files:
                print(f"      - {blank_file.name}")
        else:
            print("   ‚ÑπÔ∏è Aucun fichier blank trouv√© dans data/input/blanks/")

        print("\nüìÅ Recherche des √©chantillons...")
        samples_dir = Config.PATHS.INPUT_SAMPLES
        sample_files = list(samples_dir.glob("*.parquet"))

        if not sample_files:
            raise ValueError("Aucun fichier d'√©chantillon trouv√©.")
            
        replicate_handler = ReplicateHandler()
        replicate_groups = replicate_handler.group_replicates(sample_files)
        print(f"   ‚úì {len(replicate_groups)} √©chantillons trouv√©s:")
        for base_name, replicates in replicate_groups.items():
            print(f"      - {base_name}: {len(replicates)} r√©plicat(s)")

        # 5. Traitement des blanks
        print("\n" + "="*80)
        print("TRAITEMENT DES BLANKS")
        print("=" * 80)
        
        blank_peaks = pd.DataFrame()
        if blank_files:
            blank_peaks = blank_processor.process_blank_with_replicates(
                blank_files[0].stem,
                blank_files,
                Config.PATHS.INTERMEDIATE_DIR / "blanks"
            )

# 6. Traitement des √©chantillons
        print("\n" + "="*80)
        print("TRAITEMENT DES √âCHANTILLONS")
        print("=" * 80)

        results = process_samples_parallel(
            replicate_groups,
            blank_peaks,
            calibrator,
            Config.PATHS.INTERMEDIATE_SAMPLES
        )
        
        # V√©rifier si des √©chantillons ont √©t√© trait√©s avec succ√®s
        if not any(r.success for r in results.values()):
            raise Exception("Aucun √©chantillon n'a √©t√© trait√© avec succ√®s")

        print("\n" + "="*80)
        print("ALIGNEMENT DES FEATURES")
        print("="*80)
        
        # 7. Feature Matrix et identification
        print("\nüìä Cr√©ation de la matrice des features...")
        feature_processor.create_feature_matrix(
            input_dir=Config.PATHS.INTERMEDIATE_SAMPLES,
            output_dir=Config.PATHS.OUTPUT_DIR / "feature_matrix",
            identifier=identifier
        )

        print("\n" + "="*80)
        print("G√âN√âRATION DES VISUALISATIONS")
        print("="*80)
        
        # 8. Visualisations
        output_dir = Config.PATHS.OUTPUT_DIR
        generate_visualizations(output_dir)
        
        print("\n" + "="*80)
        print("ANALYSE DES SIMILARIT√âS")
        print("="*80)
        
        print("\nüìä Analyse des clusters d'√©chantillons...")
        analyze_and_save_clusters(output_dir)

        print("\n" + "="*80)
        print("ANALYSE DES CAT√âGORIES")
        print("="*80)
    
        print("\nüìä Analyse des cat√©gories de mol√©cules...")
        analyze_categories(output_dir)

        # Affichage du r√©capitulatif
        total_time = time.time() - start_time
        minutes = int(total_time // 60)
        seconds = int(total_time % 60)
        
        print("\n" + "="*80)
        print(" ‚úÖ FIN DU TRAITEMENT")
        print("="*80)
        print("\n Pipeline d'analyse termin√©e avec succ√®s")
        if minutes > 0:
            print(f"   ‚Ä¢ Temps de calcul total: {minutes} min {seconds} sec")
        else:
            print(f"   ‚Ä¢ Temps de calcul total: {seconds} sec")
        print(f"   ‚Ä¢ {len(replicate_groups)} √©chantillons trait√©s")
        print("=" * 80)

    except Exception as e:
        print("\n‚ùå ERREUR DANS LA PIPELINE")
        logger.error(f"Erreur pipeline : {str(e)}")
        raise

if __name__ == "__main__":
    main()